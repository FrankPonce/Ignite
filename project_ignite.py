# -*- coding: utf-8 -*-
"""Project_Ignite.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ijEpX13Co6xafwtIh3-kWPuTItYXGiC

# Rating Professors

### Data Preprocessing

Currently, each column is filled with string representations of dictionaries, which is not an ideal format for data analysis or machine learning. We need to preprocess this data into a structured form where each column represents a feature of the data, such as course_id, Quality, Difficulty, etc.
"""

import pandas as pd
import json
import streamlit as st

# Load the JSON data
with open('all_reviews.json') as file:
    data = json.load(file)

# Flatten the data into a list of dictionaries, each representing a single review
flat_data = [review for sublist in data for review in sublist]

# Convert the list of dictionaries into a DataFrame
reviews_df = pd.DataFrame(flat_data)

# Print the names of the columns
print(reviews_df.columns)

# Display the first few rows of the DataFrame to verify its structure
reviews_df.head()

import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import string

nltk.download('popular')

# Preprocessing Text Data
def clean_text(text):
    # Tokenize text
    tokens = word_tokenize(text.lower())
    # Remove punctuation
    tokens = [word for word in tokens if word.isalpha()]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if not w in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return ' '.join(tokens)

# Handle Missing Values

# For numerical columns, convert to float and replace NaN with the column mean
numerical_cols = ['Quality', 'Difficulty']
for col in numerical_cols:
    reviews_df[col] = pd.to_numeric(reviews_df[col], errors='coerce')
    reviews_df[col].fillna(reviews_df[col].mean(), inplace=True)

# For categorical/text columns, replace NaN with a placeholder
categorical_cols = ['For Credit', 'Would Take Again', 'Textbook']
for col in categorical_cols:
    reviews_df[col].fillna('Unknown', inplace=True)

print(reviews_df.columns)

# Encode Categorical Data
# One-hot encoding for the 'department' column
reviews_df = pd.get_dummies(reviews_df, columns=['department'])

# Display the first few rows to verify changes
reviews_df.head()



from textblob import TextBlob

# Sentiment Analysis with TextBlob
reviews_df['sentiment'] = reviews_df['Comment'].apply(lambda comment: TextBlob(comment).sentiment.polarity)

# Aggregate Features
# Define the columns for which we want to calculate aggregate statistics
columns_to_aggregate = ['Quality', 'Difficulty', 'sentiment']

# Group by professor and aggregate with mean, median, and std
aggregated_features = reviews_df.groupby('professor')[columns_to_aggregate].agg(['mean', 'median', 'std']).reset_index()

# Flattening the MultiIndex for easier column access
aggregated_features.columns = ['_'.join(col).strip() for col in aggregated_features.columns.values]

# Display the aggregated features to verify
print(aggregated_features.head())

print(aggregated_features.columns)

def sentiment_category(score):
    if score > 0:
        return 'Positive'
    elif score < 0:
        return 'Negative'
    else:
        return 'Neutral'

# Categorize sentiment
reviews_df['sentiment_category'] = reviews_df['sentiment'].apply(sentiment_category)

# Calculate the percentage of each sentiment category per professor
sentiment_distribution = reviews_df.groupby('professor')['sentiment_category'].value_counts(normalize=True).unstack().fillna(0)

# Multiply by 100 for percentage representation and reset index for readability
sentiment_distribution = (sentiment_distribution * 100).reset_index()

# Display the sentiment distribution for each professor
print(sentiment_distribution)




# Replace NaN values in 'Grade' with the most common grade for each professor
reviews_df['Grade'] = reviews_df.groupby('professor')['Grade'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else "Unknown"))

# Verify the handling of missing values
print(reviews_df[['professor', 'Grade']].head())


from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report

# Ensure 'Grade' column is categorical and not missing any values
reviews_df = reviews_df[reviews_df['Grade'].notnull()]

# Split the data into training and testing sets
X = reviews_df[['Comment', 'Quality', 'Difficulty']]
y = reviews_df['Grade']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Set up the preprocessing for numerical features
numerical_features = ['Quality', 'Difficulty']
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Set up the preprocessing for textual features
text_features = 'Comment'
text_transformer = TfidfVectorizer(stop_words='english')

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('text', text_transformer, text_features)
    ])

# Create the classifier pipeline
classifier = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs'))
])

# Train the model
classifier.fit(X_train, y_train)

# Predict and evaluate the model
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))


from sklearn.utils import resample

# Identify minority classes with one instance
minority_classes = y_train.value_counts()[y_train.value_counts() == 1].index

# Upsample minority classes in the training data
Xy_train = pd.concat([X_train, y_train], axis=1)
for grade in minority_classes:
    class_data = Xy_train[Xy_train['Grade'] == grade]
    # Upsample the class by duplicating its instance. Adjust n_samples as needed.
    upsampled_data = resample(class_data, replace=True, n_samples=5, random_state=42)
    Xy_train = pd.concat([Xy_train, upsampled_data])

X_train_upsampled = Xy_train[['Comment', 'Quality', 'Difficulty']]
y_train_upsampled = Xy_train['Grade']

# Now proceed with your SMOTE and model training
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Preprocessing as before
numerical_features = ['Quality', 'Difficulty']
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

text_features = 'Comment'
text_transformer = TfidfVectorizer(stop_words='english')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('text', text_transformer, text_features)
    ])

# Create the classifier pipeline with SMOTE
classifier = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('smote', SMOTE(k_neighbors=2, random_state=42)),
    ('classifier', LogisticRegression(class_weight='balanced', max_iter=1000, multi_class='multinomial', solver='lbfgs'))
])

# Train the model with the upsampled training data
classifier.fit(X_train_upsampled, y_train_upsampled)

# Predict and evaluate the model
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))



import pandas as pd
import numpy as np
from textblob import TextBlob

# Ensuring numeric columns are of a numeric dtype
reviews_df['Quality'] = pd.to_numeric(reviews_df['Quality'], errors='coerce')
reviews_df['Difficulty'] = pd.to_numeric(reviews_df['Difficulty'], errors='coerce')
reviews_df['Would Take Again'] = pd.to_numeric(reviews_df['Would Take Again'], errors='coerce')
reviews_df['sentiment'] = pd.to_numeric(reviews_df['sentiment'], errors='coerce')

# Convert 'Would Take Again' to a numerical format for aggregation
reviews_df['Would Take Again'] = reviews_df['Would Take Again'].replace({'Yes': 1, 'No': 0})

# Convert 'Grade' to a numerical format for aggregation
grade_mapping = {
    'A+': 4.3, 'A': 4.0, 'A-': 3.7,
    'B+': 3.3, 'B': 3.0, 'B-': 2.7,
    'C+': 2.3, 'C': 2.0, 'C-': 1.7,
    'D+': 1.3, 'D': 1.0, 'F': 0,
    'Drop/Withdrawal': np.nan,
    'Incomplete': np.nan,
    'Not sure yet': np.nan,
    'Not_Sure_Yet': np.nan,
    'Rather not say': np.nan,
    'Rather_Not_Say': np.nan,
    'Unknown': np.nan
}

# Apply the mapping to the 'Grade' column
reviews_df['Grade_Numerical'] = reviews_df['Grade'].map(grade_mapping)

# Fill NaN values
reviews_df['Grade_Numerical'].fillna(reviews_df['Grade_Numerical'].mean(), inplace=True)
reviews_df['Quality'].fillna(reviews_df['Quality'].mean(), inplace=True)
reviews_df['Difficulty'].fillna(reviews_df['Difficulty'].mean(), inplace=True)
reviews_df['Would Take Again'].fillna(0, inplace=True)
reviews_df['sentiment'].fillna(0, inplace=True) # Neutral sentiment if missing

# Aggregate features by professor, also considering course if we need to filter by it later
aggregated_features = reviews_df.groupby(['professor', 'course_id']).agg({
    'Quality': 'mean',
    'Difficulty': 'mean',
    'Would Take Again': 'mean',
    'sentiment': 'mean',
    'Grade_Numerical': 'mean'
}).reset_index()

def get_best_professor(department=None, course_id=None, df=aggregated_features):
    if course_id:
        df = df[df['course_id'] == course_id]

    if df.empty:
        # If no matches found, return a message indicating this
        return f"No data found for the given criteria."

    # Calculate the composite score
    df['composite_score'] = (df['Quality'] * 2 + df['Would Take Again'] * 1.5 +
                             df['sentiment'] * 1 - df['Difficulty'] * 2 +
                             df['Grade_Numerical'] * 1)

    # Get the best professor based on the highest composite score
    best_professor = df.sort_values(by='composite_score', ascending=False).iloc[0]

    return best_professor[['professor']]


# Now, let's integrate Streamlit for the user interface
st.title("Find the Best Professor for Your Course")

# Flatten the data to get a list of unique course IDs for the dropdown
flat_data = [review for sublist in data for review in sublist]
reviews_df = pd.DataFrame(flat_data)

# Extract unique course IDs and sort them
course_ids = sorted(reviews_df['course_id'].unique())

# Create a dropdown for course selection
selected_course_id = st.selectbox("Select a Course", course_ids)

# When a course is selected, display the best professor for that course
if st.button("Find Best Professor"):
    best_professor = get_best_professor(course_id=selected_course_id)
    professor_name = best_professor.iloc[0]  # Added .strip() to remove any leading/trailing spaces
    st.write(f"The best professor for {selected_course_id} is: {professor_name}")


# Example
#best_prof_for_course = get_best_professor(course_id=' FHS01')

#print("Best Professor for Course:", best_prof_for_course)